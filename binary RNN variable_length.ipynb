{"cells":[{"cell_type":"code","source":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","metadata":{"tags":[],"cell_id":"b4e829a8edc1447aa66cc48e1fff538b","source_hash":"8ccb6f9f","execution_start":1671183893035,"execution_millis":15245714,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"\n# Approach 1: binary classification using an RNN on raw, variable-length data\nWe use the annual data of each subject to predict whether his/her scores on `seven`, `bwcount` and `recall` goes up or down. \n\nThe number of waves varies from subject to subject. So we will feed each subject's data into an RNN, which is a neural network that can handle variable-length sequences. The expected output is the final year's cognitive score minus the first year's cognitive score. Since there are three cognitive scores, we will train models separately on the three scores. \n","metadata":{"tags":[],"cell_id":"0e9f03139e9b43cb91b02e11eb842e44","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Prepare the Y's ","metadata":{"tags":[],"cell_id":"1fe813215dbc4ddba38e619f9a12ab83","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"}},{"cell_type":"code","source":"import pickle\nimport numpy","metadata":{"tags":[],"cell_id":"69b8ce854c03477ba5b204167ea551db","source_hash":"fc9d8bb2","execution_start":1671183893084,"execution_millis":15245728,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":2},{"cell_type":"code","source":"[X, Y] = pickle.load(open('XY.pickle', 'br'))\n ","metadata":{"tags":[],"cell_id":"f46f070bc88d454e91ed223fd60b0046","source_hash":"e44ea311","execution_start":1671183893084,"execution_millis":305,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":3},{"cell_type":"code","source":"Y[0]\nY[0][-1, 1]\nY[0][0, 1]","metadata":{"tags":[],"cell_id":"e5d517b9f87b452ebdc3d181c0c7e2a2","source_hash":"9ff936cb","execution_start":1671183893406,"execution_millis":55,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"array([[0.3 , 0.8 , 1.  ],\n       [0.6 , 0.8 , 1.  ],\n       [0.35, 0.4 , 1.  ],\n       [0.2 , 0.2 , 1.  ]])"},"metadata":{}},{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"0.2"},"metadata":{}},{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"0.8"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"def prepare_one_y(Y, target_index: int):\n    \"\"\"Separate the target columns and generate the target for an approach \n\n    Y: 1D list of 2D numpy arrays \n    \"\"\"\n    y = [subjectY[-1, target_index] - subjectY[0, target_index] for subjectY in Y]\n    y = numpy.array(y)\n    y = numpy.heaviside(y, 0)\n    return y \n\ny = prepare_one_y(Y, 0)","metadata":{"tags":[],"cell_id":"4a2ef90269a949fd87df1022627d9111","source_hash":"f9896d75","execution_start":1671183893504,"execution_millis":0,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":5},{"cell_type":"code","source":"total_subjects = len(Y)\nmetric_names = [\"recall\", \"seven\", \"bwcount\"]\nfor target_index in [0,1,2]:\n    y = prepare_one_y(Y, target_index)\n    number_of_increasing_subjects = numpy.count_nonzero(y)\n    percentage_of_increasing_subjects = number_of_increasing_subjects/total_subjects*100\n    metric_name = metric_names[target_index]\n    print (f'{number_of_increasing_subjects} out of {total_subjects}, \\\n    or {percentage_of_increasing_subjects:.2f}% show end-to-end improvement \\\n    on metric {metric_name}')","metadata":{"tags":[],"cell_id":"7b8deee76a5b48c8a3eec9542ecbe814","source_hash":"21e1f9de","execution_start":1671183893551,"execution_millis":127,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"9216 out of 26178,     or 35.21% show end-to-end improvement     on metric recall\n6167 out of 26178,     or 23.56% show end-to-end improvement     on metric seven\n1206 out of 26178,     or 4.61% show end-to-end improvement     on metric bwcount\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## build the network","metadata":{"tags":[],"cell_id":"e91aa7d66f0846cbabd2a537c205fc97","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"X[0].shape","metadata":{"tags":[],"cell_id":"6ac3ba2f029b43a8beea30484d93088f","source_hash":"f607d432","execution_start":1671183893741,"execution_millis":15245936,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"(4, 23)"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"import collections\ncollections.Counter(map(len, Y))","metadata":{"tags":[],"cell_id":"af96639e07fc4724b774dd3bf4893a16","source_hash":"56f91806","execution_start":1671183893742,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"Counter({4: 3600, 5: 5115, 6: 3624, 7: 5562, 3: 2348, 2: 5929})"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"import torch.nn as nn\nimport torch\n\nclass Net(nn.Module):\n  # cannot be batched as input sequence lengths are different \n  def __init__(self, input_size, hidden_size, num_layers):\n      super().__init__()\n      self.rnn = torch.nn.RNN(input_size, hidden_size, num_layers)\n    #   self.fc1 = torch.nn.Linear(hidden_size, 3)\n      self.fc1 = torch.nn.Linear(hidden_size, 1)\n      self.fc2 = torch.nn.Linear(3, 1)\n\n  def forward(self, x):\n      output, hn = self.rnn(x)\n      x = self.fc1(hn[0])\n    #   x = self.fc2(x)\n      x = torch.sigmoid(x)\n      return x\n\nnet = Net(23, 5, 2)","metadata":{"tags":[],"cell_id":"45ce8ab7f95c4590bdf00ed73fc0a138","source_hash":"a6b51d20","execution_start":1671183993815,"execution_millis":3,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# test 1 \nnet = Net(23, 5, 2)\nx = torch.randn(7, 23)\nnet(x)\n# x","metadata":{"tags":[],"cell_id":"7f1450b765bc4e07b46ff4052593764f","source_hash":"fc55d734","execution_start":1671184349645,"execution_millis":1,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":31,"data":{"text/plain":"tensor([0.4182], grad_fn=<SigmoidBackward0>)"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"# test 2 \n# we run into a weird error see the cell below. So we have to do this workaround. \n# CREATE a new cell\n# net = Net(23, 5, 2)\n# net = net.float()\nnet(torch.from_numpy(X[2000]).float())","metadata":{"tags":[],"cell_id":"f36a77674f1f45e9acb154fac935a35f","source_hash":"6a77a65f","execution_start":1671183999847,"execution_millis":18,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"tensor([0.5056], grad_fn=<SigmoidBackward0>)"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"# why this didn't work? \nnet = Net(23, 5, 2) # init a fresh one\nnet(torch.from_numpy(X[0]).float())","metadata":{"tags":[],"cell_id":"c9314287de934c9e83c24cd51155e04d","source_hash":"6718202c","execution_start":1671183965446,"execution_millis":44,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"tensor([0.4664], grad_fn=<SigmoidBackward0>)"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"# Convert X to cope with the error \n# again, we cannot do batch due to a weird issue \nnew_X = [torch.from_numpy(x).float() for x in X]","metadata":{"tags":[],"cell_id":"b0da34e5abd748b295d24f5666213353","source_hash":"71a8ae13","output_cleared":true,"execution_start":1671183896147,"execution_millis":377,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## The trainer","metadata":{"tags":[],"cell_id":"d0cec71cd9a5490190ec6a06c5083f15","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"def train(net, X, Y, target_index, learning_rate, momentum):\n    \"\"\"\n\n    net: a torch.nn instance\n    target_index: integer 0, 1, 2 (recall, seven, bwcount)\n    \"\"\"\n    import torch.optim as optim\n\n    criterion = nn.BCELoss()\n    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)\n\n    optimizer.zero_grad()    # zero the parameter gradients\n\n    y = prepare_one_y(Y, target_index)\n    y = torch.tensor(y).float() # workaround for the weird problem \n\n    batch_size = 2000 \n    print_batch = 2000\n\n    loss_log = [] \n\n    for epoch in range(5):  # loop over the dataset multiple times\n        running_loss = 0.0\n        for i, inputs in enumerate(X, 1):\n            labels = y[i-1]\n\n            optimizer.zero_grad()  # should not be here.\n\n            # forward + backward + optimize\n            prediction = net(inputs)\n            prediction = prediction[0]\n\n            loss = criterion(prediction, labels)\n            # loss /= batch_size # normalize the loss, may not needed \n\n            loss.backward()        \n\n            # print statistics\n            running_loss += loss.item()\n            if i % batch_size == 0:    #every mini-batches\n                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / batch_size:.3f}')\n                loss_log.append(running_loss / batch_size)\n                optimizer.step()\n                running_loss = 0.0\n                \n            # running_loss += loss.item()\n            # if i % print_batch == 0: \n            #     print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / print_batch:.3f}')\n            #     running_loss = 0.0\n\n    return loss_log","metadata":{"tags":[],"cell_id":"fafae7bafa914710967885b5c952cc34","source_hash":"aae3afda","execution_start":1671183896535,"execution_millis":15246825,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"### Training log for `bw_count` (target_index =2)\nSlow loss convergence with high initial loss (0.9): `optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.5)`\nSlow loss convergence with low inital loss (0.7): `optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)`","metadata":{"tags":[],"cell_id":"1c0d6ef7a0ba4502986be0c8da84e8d4","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"net = Net(23, 5, 2)\nloss_log_005_09 = train(net, new_X, Y, 2, 0.05, 0.9)","metadata":{"tags":[],"cell_id":"ae17e7d602234d0e87e4a48b734948c3","source_hash":"9e0b3b70","execution_start":1671183896595,"execution_millis":32072,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"[1,  2001] loss: 0.855\n[1,  4001] loss: 0.787\n[1,  6001] loss: 0.684\n[1,  8001] loss: 0.550\n[1, 10001] loss: 0.423\n[1, 12001] loss: 0.335\n","output_type":"stream"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn [15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m net \u001b[38;5;241m=\u001b[39m Net(\u001b[38;5;241m23\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m loss_log_005_09 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn [14], line 36\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(net, X, Y, target_index, learning_rate, momentum)\u001b[0m\n\u001b[1;32m     33\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(prediction, labels)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# loss /= batch_size # normalize the loss, may not needed \u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m        \n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# print statistics\u001b[39;00m\n\u001b[1;32m     39\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"execution_count":15},{"cell_type":"markdown","source":"### Training log for `seven` (target_index=1)\n","metadata":{"tags":[],"cell_id":"2570209a98574480bde3e580d8c3c921","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"net = Net(23, 5, 2)\nloss_log_005_09 = train(net, new_X, Y, 1, 0.05, 0.9)","metadata":{"tags":[],"cell_id":"b5ef7544073b453790258b7af475e681","source_hash":"4c495a7e","execution_start":1671168809100,"execution_millis":319720,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"[1,  2001] loss: 0.673\n[1,  4001] loss: 0.653\n[1,  6001] loss: 0.612\n[1,  8001] loss: 0.575\n[1, 10001] loss: 0.563\n[1, 12001] loss: 0.497\n[1, 14001] loss: 0.472\n[1, 16001] loss: 0.608\n[1, 18001] loss: 0.647\n[1, 20001] loss: 0.745\n[1, 22001] loss: 0.714\n[1, 24001] loss: 0.736\n[1, 26001] loss: 0.786\n[2,  2001] loss: 0.548\n[2,  4001] loss: 0.529\n[2,  6001] loss: 0.512\n[2,  8001] loss: 0.519\n[2, 10001] loss: 0.549\n[2, 12001] loss: 0.492\n[2, 14001] loss: 0.470\n[2, 16001] loss: 0.591\n[2, 18001] loss: 0.599\n[2, 20001] loss: 0.649\n[2, 22001] loss: 0.603\n[2, 24001] loss: 0.603\n[2, 26001] loss: 0.629\n[3,  2001] loss: 0.519\n[3,  4001] loss: 0.541\n[3,  6001] loss: 0.561\n[3,  8001] loss: 0.577\n[3, 10001] loss: 0.586\n[3, 12001] loss: 0.529\n[3, 14001] loss: 0.494\n[3, 16001] loss: 0.570\n[3, 18001] loss: 0.579\n[3, 20001] loss: 0.638\n[3, 22001] loss: 0.611\n[3, 24001] loss: 0.626\n[3, 26001] loss: 0.667\n[4,  2001] loss: 0.513\n[4,  4001] loss: 0.508\n[4,  6001] loss: 0.507\n[4,  8001] loss: 0.516\n[4, 10001] loss: 0.546\n[4, 12001] loss: 0.497\n[4, 14001] loss: 0.472\n[4, 16001] loss: 0.587\n[4, 18001] loss: 0.596\n[4, 20001] loss: 0.657\n[4, 22001] loss: 0.624\n[4, 24001] loss: 0.632\n[4, 26001] loss: 0.681\n[5,  2001] loss: 0.532\n[5,  4001] loss: 0.526\n[5,  6001] loss: 0.532\n[5,  8001] loss: 0.564\n[5, 10001] loss: 0.608\n[5, 12001] loss: 0.537\n[5, 14001] loss: 0.513\n[5, 16001] loss: 0.680\n[5, 18001] loss: 0.674\n[5, 20001] loss: 0.789\n[5, 22001] loss: 0.753\n[5, 24001] loss: 0.763\n[5, 26001] loss: 0.882\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"### Training log for `recall` (target_index=0)\n","metadata":{"tags":[],"cell_id":"ba5401871b4b4e538640814ef934305c","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"net = Net(23, 5, 2)\nloss_log_005_09 = train(net, new_X, Y, 0, 0.05, 0.9)","metadata":{"tags":[],"cell_id":"fedd378e7da6420f87755109a9868c02","source_hash":"3a5fe592","execution_start":1671169128818,"execution_millis":357221,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"[1,  2001] loss: 0.683\n[1,  4001] loss: 0.672\n[1,  6001] loss: 0.640\n[1,  8001] loss: 0.609\n[1, 10001] loss: 0.634\n[1, 12001] loss: 0.563\n[1, 14001] loss: 0.547\n[1, 16001] loss: 0.694\n[1, 18001] loss: 0.811\n[1, 20001] loss: 0.916\n[1, 22001] loss: 1.041\n[1, 24001] loss: 1.151\n[1, 26001] loss: 1.072\n[2,  2001] loss: 0.714\n[2,  4001] loss: 0.764\n[2,  6001] loss: 0.759\n[2,  8001] loss: 0.757\n[2, 10001] loss: 0.863\n[2, 12001] loss: 0.635\n[2, 14001] loss: 0.609\n[2, 16001] loss: 0.891\n[2, 18001] loss: 1.015\n[2, 20001] loss: 1.067\n[2, 22001] loss: 1.173\n[2, 24001] loss: 1.213\n[2, 26001] loss: 1.040\n[3,  2001] loss: 0.695\n[3,  4001] loss: 0.728\n[3,  6001] loss: 0.713\n[3,  8001] loss: 0.687\n[3, 10001] loss: 0.780\n[3, 12001] loss: 0.592\n[3, 14001] loss: 0.579\n[3, 16001] loss: 0.801\n[3, 18001] loss: 0.891\n[3, 20001] loss: 0.933\n[3, 22001] loss: 1.084\n[3, 24001] loss: 1.170\n[3, 26001] loss: 1.040\n[4,  2001] loss: 0.741\n[4,  4001] loss: 0.809\n[4,  6001] loss: 0.815\n[4,  8001] loss: 0.795\n[4, 10001] loss: 0.978\n[4, 12001] loss: 0.720\n[4, 14001] loss: 0.697\n[4, 16001] loss: 1.104\n[4, 18001] loss: 1.281\n[4, 20001] loss: 1.336\n[4, 22001] loss: 1.564\n[4, 24001] loss: 1.625\n[4, 26001] loss: 1.364\n[5,  2001] loss: 0.917\n[5,  4001] loss: 0.995\n[5,  6001] loss: 0.980\n[5,  8001] loss: 0.935\n[5, 10001] loss: 1.181\n[5, 12001] loss: 0.859\n[5, 14001] loss: 0.830\n[5, 16001] loss: 1.346\n[5, 18001] loss: 1.553\n[5, 20001] loss: 1.606\n[5, 22001] loss: 1.890\n[5, 24001] loss: 1.944\n[5, 26001] loss: 1.628\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"tags":[],"cell_id":"4d20d670da144b2d8b3a5e20f5882bbe","source_hash":"b623e53d","execution_start":1671169486045,"execution_millis":52,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=38ce621b-4696-4047-8b25-0501b493ce55' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"tags":[],"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"69898ed7559443ba878aac5b92292d82","deepnote_execution_queue":[]}}