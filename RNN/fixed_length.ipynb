{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import numpy \n",
    "import sys     \n",
    "sys.path.append(\"../\") # add parent directory to path\n",
    "import preprocessing # this project\n",
    "import typing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/forrest/repos/AgeML/RNN/../preprocessing.py:196: DtypeWarning: Columns (5,24,28,29,30,31,32,33,34,61,62) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pandas.read_csv(csv_file)\n",
      "100%|██████████| 29648/29648 [00:07<00:00, 4025.47it/s]\n",
      "100%|██████████| 26178/26178 [00:29<00:00, 875.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 18) (4, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# just load data so functions can get test inputs \n",
    "# no need to go into final code \n",
    "\n",
    "[X, Y] = preprocessing.main(\n",
    "    use_feature_categories=[\"socio-demographics\", \"health\", \"social\", \"neighbourhood\"],\n",
    "    csv_file=\"../ML_social.csv\", dump=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slide_over_one_subject(subject_X, subject_Y, minimal_length):\n",
    "    \"\"\"Create samples from the data of one subject using a sliding window\n",
    "\n",
    "    subject_X, subject_Y: 2D numpy array, rows for waves and columns for features\n",
    "    subject_new_X, subject_new_Y: 3D numpy array, axis 0 for sliding window, axis 1 for waves, axis 2 for features\n",
    "\n",
    "    \"\"\"\n",
    "    subject_new_X, subject_new_Y = [], []\n",
    "    num_features = subject_X.shape[1]\n",
    "    subject_X_windowed = numpy.lib.stride_tricks.sliding_window_view(\n",
    "        subject_X, \n",
    "        window_shape=(minimal_length, num_features)\n",
    "    ) \n",
    "    subject_new_X = subject_X_windowed[:, -1, :, :]\n",
    "\n",
    "    num_targets = subject_Y.shape[1]\n",
    "    subject_Y_windowed = numpy.lib.stride_tricks.sliding_window_view(\n",
    "        subject_Y, \n",
    "        window_shape=(minimal_length, num_targets)\n",
    "    ) \n",
    "    subject_new_Y = subject_Y_windowed[:, -1, :, :]\n",
    "\n",
    "    return subject_new_X, subject_new_Y\n",
    "\n",
    "def sampling_fixed_length(X, Y, minimal_length):\n",
    "    new_X, new_Y = [], []\n",
    "    for subject_X, subject_Y in zip(X, Y):\n",
    "        if len(subject_Y)>= minimal_length:\n",
    "            subject_new_X, subject_new_Y = slide_over_one_subject(subject_X, subject_Y, minimal_length=minimal_length)\n",
    "            new_X += subject_new_X.tolist()\n",
    "            new_Y += subject_new_Y.tolist()\n",
    "\n",
    "    new_X, new_Y = numpy.array(new_X), numpy.array(new_Y)\n",
    "\n",
    "    return new_X, new_Y \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.44444444, 0.3       , 0.8       , 1.        ],\n",
      "       [0.66666667, 0.6       , 0.8       , 1.        ],\n",
      "       [0.40740741, 0.35      , 0.4       , 1.        ],\n",
      "       [0.25925926, 0.2       , 0.2       , 1.        ]]), array([[0.66666667, 0.55      , 1.        , 1.        ],\n",
      "       [0.59259259, 0.5       , 0.8       , 1.        ],\n",
      "       [0.62962963, 0.5       , 1.        , 1.        ],\n",
      "       [0.51851852, 0.4       , 0.8       , 1.        ],\n",
      "       [0.62962963, 0.5       , 1.        , 1.        ]])]\n",
      "[[[0.44444444 0.3        0.8        1.        ]\n",
      "  [0.66666667 0.6        0.8        1.        ]\n",
      "  [0.40740741 0.35       0.4        1.        ]\n",
      "  [0.25925926 0.2        0.2        1.        ]]\n",
      "\n",
      " [[0.66666667 0.55       1.         1.        ]\n",
      "  [0.59259259 0.5        0.8        1.        ]\n",
      "  [0.62962963 0.5        1.         1.        ]\n",
      "  [0.51851852 0.4        0.8        1.        ]]\n",
      "\n",
      " [[0.59259259 0.5        0.8        1.        ]\n",
      "  [0.62962963 0.5        1.         1.        ]\n",
      "  [0.51851852 0.4        0.8        1.        ]\n",
      "  [0.62962963 0.5        1.         1.        ]]]\n",
      "[[0.3  0.2 ]\n",
      " [0.55 0.4 ]\n",
      " [0.5  0.5 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# test the work on Y \n",
    "_, new_Y = sampling_fixed_length(X[:2], Y[:2], minimal_length=4) \n",
    "print (Y[:2])\n",
    "print (new_Y)\n",
    "print (new_Y[:, [0, -1], 1])\n",
    "numpy.heaviside(new_Y[:, -1, 1]-new_Y[:, 0, 1], 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_labels(Y, target_index: int):\n",
    "    \"\"\"Extract the training label for a particular target dimension \n",
    "\n",
    "    Y: 3D ndarray, axis 0 is sample, axis 1 is wave, axis 2 is score \n",
    "    \"\"\"\n",
    "    # y_of_interest = Y[:, :, target_index] # 2D array \n",
    "    begin_score = Y[:, 0, target_index]\n",
    "    end_score = Y[:, -1, target_index]\n",
    "\n",
    "    labels = end_score - begin_score\n",
    "    labels = numpy.heaviside(labels, 0)\n",
    "\n",
    "    return labels.reshape((-1,1)) # 2d arrary, Nx1\n",
    "\n",
    "# top level function for training data preparation \n",
    "# need to rerun for each target index rangeing from 0 to 2 \n",
    "def prepare_training_data(X, Y, target_index: int, minimal_length):\n",
    "    new_X, new_Y = sampling_fixed_length(X, Y, minimal_length=minimal_length) \n",
    "    new_Y = generate_labels(new_Y, target_index=target_index)\n",
    "    return new_X, new_Y \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data so we can test the model\n",
    "# No need to go into final code \n",
    "train_X, train_y = prepare_training_data(X, Y, target_index=0, minimal_length=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class RNN(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, num_layers):\n",
    "      super().__init__()\n",
    "      self.rnn = torch.nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "    #   self.fc1 = torch.nn.Linear(hidden_size, 3)\n",
    "      self.fc1 = torch.nn.Linear(hidden_size, 1)\n",
    "\n",
    "  def forward(self, x):\n",
    "      output, hn = self.rnn(x)\n",
    "      x = self.fc1(hn[0])\n",
    "    #   x = self.fc2(x)\n",
    "      x = torch.sigmoid(x)\n",
    "      return x\n",
    "\n",
    "class FC(nn.Module):\n",
    "  def __init__(self, input_size):\n",
    "      super().__init__()\n",
    "      self.fc1 = torch.nn.Linear(input_size, 50)\n",
    "      self.fc2 = torch.nn.Linear(50, 20)\n",
    "      self.fc3 = torch.nn.Linear(20, 10)\n",
    "      self.fc4 = torch.nn.Linear(10, 1)\n",
    "\n",
    "  def forward(self, x):\n",
    "      x = torch.flatten(x, start_dim=1) # first dimension is batch \n",
    "      x = self.fc1(x)\n",
    "      x = torch.sigmoid(x)\n",
    "      x = self.fc2(x)\n",
    "      x = torch.sigmoid(x)\n",
    "      x = self.fc3(x)\n",
    "      x = torch.sigmoid(x)\n",
    "      x = self.fc4(x)\n",
    "      x = torch.sigmoid(x)\n",
    "    \n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7076],\n",
       "        [0.7131],\n",
       "        [0.7114],\n",
       "        [0.7085],\n",
       "        [0.6459],\n",
       "        [0.6243],\n",
       "        [0.6957],\n",
       "        [0.7096],\n",
       "        [0.7107],\n",
       "        [0.7075]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test \n",
    "# USING VARIABLES PREVIOUSLY LOADED INTO THE MEMORY\n",
    "\n",
    "train_X_tensor = torch.from_numpy(train_X.astype(\"float32\"))\n",
    "train_y_tensor = torch.from_numpy(train_y.astype(\"float32\"))\n",
    "\n",
    "input_dimension = train_X.shape[2]\n",
    "net = RNN(input_dimension, 5, 2)\n",
    "net(train_X_tensor[10:20]) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The trainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, X, y, learning_rate, momentum, batch_size, print_batch_size):\n",
    "    \"\"\"\n",
    "\n",
    "    net: a torch.nn instance\n",
    "    X: 3D PyTorch Tensor, [sample, wave, feature]\n",
    "    y: 2D PyTorch Tensor, Nx1, [sample, binary label]\n",
    "    \"\"\"\n",
    "    import torch.optim as optim\n",
    "\n",
    "    criterion = nn.BCELoss() \n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    batch_size = batch_size\n",
    "    print_batch_size = print_batch_size\n",
    "\n",
    "    loss_log = [] \n",
    "\n",
    "    for epoch in range(3):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i in range(0, X.shape[0], batch_size):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch_X, batch_y = X[i:i+batch_size], y[i: i+batch_size]\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            prediction = net.forward(batch_X)\n",
    "            loss = criterion(prediction, batch_y)\n",
    "            \n",
    "            # update weights at the end of each batch\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "\n",
    "            # log loss\n",
    "            loss_batch = loss.item()\n",
    "            running_loss += loss_batch # accumulated loss of many batches until print \n",
    "            loss_log.append(loss_batch)\n",
    "\n",
    "            # print statistics\n",
    "            if i % print_batch_size == 0 and i / print_batch_size > 1:\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / (print_batch_size/batch_size):.7f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "    return loss_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 25601] loss: 1.2685060\n",
      "[1, 38401] loss: 0.6194438\n",
      "[2, 25601] loss: 1.2657085\n",
      "[2, 38401] loss: 0.6191720\n",
      "[3, 25601] loss: 1.2645060\n",
      "[3, 38401] loss: 0.6185363\n"
     ]
    }
   ],
   "source": [
    "# Test the network training \n",
    "# USING VARIABLES PREVIOUSLY PLACE IN THE NOTEBOOK\n",
    "\n",
    "input_dimension = train_X_tensor.shape[2]\n",
    "net = RNN(input_dimension, 5, 2)\n",
    "\n",
    "loss_log = train(net, train_X_tensor, train_y_tensor, \n",
    "                    learning_rate=0.01, \n",
    "                    momentum=0.9,\n",
    "                    batch_size=32, print_batch_size=32*400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_one_target_one_arch(use_feature_categories: typing.List[str], csv_path:str, \n",
    "                            target_index:int, minimal_length:int, arch:str, \n",
    "                            learning_rate:float, momentum:float, \n",
    "                            batch_size:int, print_batch_size:int):\n",
    "\n",
    "    [X, Y] = preprocessing.main(use_feature_categories= use_feature_categories,\n",
    "                csv_file=csv_path, dump=False)    \n",
    "\n",
    "    train_X, train_y = prepare_training_data(X, Y, \n",
    "                                target_index=target_index, \n",
    "                                minimal_length=4)\n",
    "\n",
    "    # numpy float 64 for PyTorch float 32\n",
    "    train_X = torch.from_numpy(train_X.astype(\"float32\"))\n",
    "    train_y = torch.from_numpy(train_y.astype(\"float32\"))\n",
    "\n",
    "    input_dimension = train_X.shape[2]\n",
    "    if arch == \"RNN\":\n",
    "        net = RNN(input_dimension, 5, 2)\n",
    "    elif arch == \"FC\":\n",
    "        net = FC(input_dimension)\n",
    "\n",
    "    loss_log = train(net, train_X, train_y, \n",
    "                     learning_rate=learning_rate, \n",
    "                     momentum=momentum,\n",
    "                     batch_size=batch_size, print_batch_size=print_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/forrest/repos/AgeML/RNN/../preprocessing.py:196: DtypeWarning: Columns (5,24,28,29,30,31,32,33,34,61,62) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pandas.read_csv(csv_file)\n",
      "100%|██████████| 29648/29648 [00:07<00:00, 4183.83it/s]\n",
      "100%|██████████| 26178/26178 [00:30<00:00, 848.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 18) (4, 4)\n",
      "[1,   641] loss: 1.4987940\n",
      "[1,   961] loss: 0.6816145\n",
      "[1,  1281] loss: 0.6550148\n",
      "[1,  1601] loss: 0.6518805\n",
      "[1,  1921] loss: 0.6256426\n",
      "[1,  2241] loss: 0.6523841\n",
      "[1,  2561] loss: 0.6342107\n",
      "[1,  2881] loss: 0.6464762\n",
      "[1,  3201] loss: 0.6457524\n",
      "[1,  3521] loss: 0.6384335\n",
      "[1,  3841] loss: 0.6361469\n",
      "[1,  4161] loss: 0.6024196\n",
      "[1,  4481] loss: 0.6382607\n",
      "[1,  4801] loss: 0.6364564\n",
      "[1,  5121] loss: 0.6012394\n",
      "[1,  5441] loss: 0.6619647\n",
      "[1,  5761] loss: 0.6275776\n",
      "[1,  6081] loss: 0.6146175\n",
      "[1,  6401] loss: 0.6561692\n",
      "[1,  6721] loss: 0.6373615\n",
      "[1,  7041] loss: 0.6207815\n",
      "[1,  7361] loss: 0.6103640\n",
      "[1,  7681] loss: 0.6402642\n",
      "[1,  8001] loss: 0.6514095\n",
      "[1,  8321] loss: 0.6123548\n",
      "[1,  8641] loss: 0.6341721\n",
      "[1,  8961] loss: 0.6271838\n",
      "[1,  9281] loss: 0.6134559\n",
      "[1,  9601] loss: 0.6192672\n",
      "[1,  9921] loss: 0.6035549\n",
      "[1, 10241] loss: 0.6277377\n",
      "[1, 10561] loss: 0.6197450\n",
      "[1, 10881] loss: 0.5947810\n",
      "[1, 11201] loss: 0.6086776\n",
      "[1, 11521] loss: 0.6133608\n",
      "[1, 11841] loss: 0.6254957\n",
      "[1, 12161] loss: 0.6193678\n",
      "[1, 12481] loss: 0.6501735\n",
      "[1, 12801] loss: 0.6387401\n",
      "[1, 13121] loss: 0.6277773\n",
      "[1, 13441] loss: 0.6069835\n",
      "[1, 13761] loss: 0.6491053\n",
      "[1, 14081] loss: 0.6356319\n",
      "[1, 14401] loss: 0.6060194\n",
      "[1, 14721] loss: 0.6275328\n",
      "[1, 15041] loss: 0.6713554\n",
      "[1, 15361] loss: 0.6330374\n",
      "[1, 15681] loss: 0.6371857\n",
      "[1, 16001] loss: 0.6437505\n",
      "[1, 16321] loss: 0.6492528\n",
      "[1, 16641] loss: 0.6329823\n",
      "[1, 16961] loss: 0.6178347\n",
      "[1, 17281] loss: 0.6169575\n",
      "[1, 17601] loss: 0.6387213\n",
      "[1, 17921] loss: 0.6092697\n",
      "[1, 18241] loss: 0.6070066\n",
      "[1, 18561] loss: 0.6584735\n",
      "[1, 18881] loss: 0.6217566\n",
      "[1, 19201] loss: 0.6291442\n",
      "[1, 19521] loss: 0.5586794\n",
      "[1, 19841] loss: 0.6256805\n",
      "[1, 20161] loss: 0.6233434\n",
      "[1, 20481] loss: 0.6202023\n",
      "[1, 20801] loss: 0.7071496\n",
      "[1, 21121] loss: 0.6709901\n",
      "[1, 21441] loss: 0.6636359\n",
      "[1, 21761] loss: 0.6857976\n",
      "[1, 22081] loss: 0.6675390\n",
      "[1, 22401] loss: 0.6472918\n",
      "[1, 22721] loss: 0.6633043\n",
      "[1, 23041] loss: 0.6762990\n",
      "[1, 23361] loss: 0.6527518\n",
      "[1, 23681] loss: 0.6580450\n",
      "[1, 24001] loss: 0.6565485\n",
      "[1, 24321] loss: 0.6395096\n",
      "[1, 24641] loss: 0.6842022\n",
      "[1, 24961] loss: 0.6468684\n",
      "[1, 25281] loss: 0.6621907\n",
      "[1, 25601] loss: 0.6768968\n",
      "[1, 25921] loss: 0.6507227\n",
      "[1, 26241] loss: 0.6428951\n",
      "[1, 26561] loss: 0.6142237\n",
      "[1, 26881] loss: 0.5106688\n",
      "[1, 27201] loss: 0.5117276\n",
      "[1, 27521] loss: 0.5284843\n",
      "[1, 27841] loss: 0.4988442\n",
      "[1, 28161] loss: 0.5121292\n",
      "[1, 28481] loss: 0.4772341\n",
      "[1, 28801] loss: 0.5270580\n",
      "[1, 29121] loss: 0.5539251\n",
      "[1, 29441] loss: 0.5191618\n",
      "[1, 29761] loss: 0.5613863\n",
      "[1, 30081] loss: 0.5202339\n",
      "[1, 30401] loss: 0.5933129\n",
      "[1, 30721] loss: 0.5012273\n",
      "[1, 31041] loss: 0.6468781\n",
      "[1, 31361] loss: 0.7048151\n",
      "[1, 31681] loss: 0.6483583\n",
      "[1, 32001] loss: 0.6942037\n",
      "[1, 32321] loss: 0.6770188\n",
      "[1, 32641] loss: 0.6784052\n",
      "[1, 32961] loss: 0.6514008\n",
      "[1, 33281] loss: 0.6875807\n",
      "[1, 33601] loss: 0.6794127\n",
      "[1, 33921] loss: 0.6668430\n",
      "[1, 34241] loss: 0.6801663\n",
      "[1, 34561] loss: 0.6804073\n",
      "[1, 34881] loss: 0.6660821\n",
      "[1, 35201] loss: 0.6806790\n",
      "[1, 35521] loss: 0.6776530\n",
      "[1, 35841] loss: 0.6892612\n",
      "[1, 36161] loss: 0.6736876\n",
      "[1, 36481] loss: 0.6726100\n",
      "[1, 36801] loss: 0.6731706\n",
      "[1, 37121] loss: 0.6562700\n",
      "[1, 37441] loss: 0.6871068\n",
      "[1, 37761] loss: 0.6669190\n",
      "[1, 38081] loss: 0.6778802\n",
      "[1, 38401] loss: 0.6827210\n",
      "[1, 38721] loss: 0.6685721\n",
      "[1, 39041] loss: 0.6801212\n",
      "[1, 39361] loss: 0.7065403\n",
      "[1, 39681] loss: 0.7034031\n",
      "[1, 40001] loss: 0.6957617\n",
      "[1, 40321] loss: 0.6880861\n",
      "[1, 40641] loss: 0.6825431\n",
      "[1, 40961] loss: 0.6891544\n",
      "[1, 41281] loss: 0.6842034\n",
      "[1, 41601] loss: 0.6970418\n",
      "[1, 41921] loss: 0.6848178\n",
      "[1, 42241] loss: 0.6769046\n",
      "[1, 42561] loss: 0.6856951\n",
      "[1, 42881] loss: 0.6809303\n",
      "[1, 43201] loss: 0.6961305\n",
      "[1, 43521] loss: 0.7024457\n",
      "[1, 43841] loss: 0.6896683\n",
      "[1, 44161] loss: 0.6873030\n",
      "[1, 44481] loss: 0.6842448\n",
      "[1, 44801] loss: 0.6765348\n",
      "[1, 45121] loss: 0.6904704\n",
      "[1, 45441] loss: 0.6963479\n",
      "[1, 45761] loss: 0.6899466\n",
      "[1, 46081] loss: 0.6926218\n",
      "[1, 46401] loss: 0.6923228\n",
      "[1, 46721] loss: 0.6919412\n",
      "[2,   641] loss: 1.3560533\n",
      "[2,   961] loss: 0.6024038\n",
      "[2,  1281] loss: 0.6102065\n",
      "[2,  1601] loss: 0.6387275\n",
      "[2,  1921] loss: 0.6018697\n",
      "[2,  2241] loss: 0.6513017\n",
      "[2,  2561] loss: 0.6214322\n",
      "[2,  2881] loss: 0.6342196\n",
      "[2,  3201] loss: 0.6411694\n",
      "[2,  3521] loss: 0.6311724\n",
      "[2,  3841] loss: 0.6289676\n",
      "[2,  4161] loss: 0.6003231\n",
      "[2,  4481] loss: 0.6364666\n",
      "[2,  4801] loss: 0.6358287\n",
      "[2,  5121] loss: 0.5944235\n",
      "[2,  5441] loss: 0.6598327\n",
      "[2,  5761] loss: 0.6247953\n",
      "[2,  6081] loss: 0.6127921\n",
      "[2,  6401] loss: 0.6544610\n",
      "[2,  6721] loss: 0.6319784\n",
      "[2,  7041] loss: 0.6174509\n",
      "[2,  7361] loss: 0.6088983\n",
      "[2,  7681] loss: 0.6387527\n",
      "[2,  8001] loss: 0.6488300\n",
      "[2,  8321] loss: 0.6102796\n",
      "[2,  8641] loss: 0.6324327\n",
      "[2,  8961] loss: 0.6139182\n",
      "[2,  9281] loss: 0.6071557\n",
      "[2,  9601] loss: 0.6177178\n",
      "[2,  9921] loss: 0.6001583\n",
      "[2, 10241] loss: 0.6280106\n",
      "[2, 10561] loss: 0.6119061\n",
      "[2, 10881] loss: 0.5938619\n",
      "[2, 11201] loss: 0.6077974\n",
      "[2, 11521] loss: 0.6143359\n",
      "[2, 11841] loss: 0.6222226\n",
      "[2, 12161] loss: 0.6148013\n",
      "[2, 12481] loss: 0.6473829\n",
      "[2, 12801] loss: 0.6379083\n",
      "[2, 13121] loss: 0.6260034\n",
      "[2, 13441] loss: 0.6058621\n",
      "[2, 13761] loss: 0.6440716\n",
      "[2, 14081] loss: 0.6341123\n",
      "[2, 14401] loss: 0.6017893\n",
      "[2, 14721] loss: 0.6258416\n",
      "[2, 15041] loss: 0.6685999\n",
      "[2, 15361] loss: 0.6308663\n",
      "[2, 15681] loss: 0.6353853\n",
      "[2, 16001] loss: 0.6411985\n",
      "[2, 16321] loss: 0.6469174\n",
      "[2, 16641] loss: 0.6314875\n",
      "[2, 16961] loss: 0.6147239\n",
      "[2, 17281] loss: 0.6118378\n",
      "[2, 17601] loss: 0.6392074\n",
      "[2, 17921] loss: 0.6079828\n",
      "[2, 18241] loss: 0.6062350\n",
      "[2, 18561] loss: 0.6564846\n",
      "[2, 18881] loss: 0.6193179\n",
      "[2, 19201] loss: 0.6275770\n",
      "[2, 19521] loss: 0.5554884\n",
      "[2, 19841] loss: 0.6250326\n",
      "[2, 20161] loss: 0.6224484\n",
      "[2, 20481] loss: 0.6238427\n",
      "[2, 20801] loss: 0.7018187\n",
      "[2, 21121] loss: 0.6704351\n",
      "[2, 21441] loss: 0.6629760\n",
      "[2, 21761] loss: 0.6847716\n",
      "[2, 22081] loss: 0.6685609\n",
      "[2, 22401] loss: 0.6460848\n",
      "[2, 22721] loss: 0.6613034\n",
      "[2, 23041] loss: 0.6753036\n",
      "[2, 23361] loss: 0.6519088\n",
      "[2, 23681] loss: 0.6577229\n",
      "[2, 24001] loss: 0.6604531\n",
      "[2, 24321] loss: 0.6374592\n",
      "[2, 24641] loss: 0.6840102\n",
      "[2, 24961] loss: 0.6441686\n",
      "[2, 25281] loss: 0.6630504\n",
      "[2, 25601] loss: 0.6746509\n",
      "[2, 25921] loss: 0.6510074\n",
      "[2, 26241] loss: 0.6420943\n",
      "[2, 26561] loss: 0.6103831\n",
      "[2, 26881] loss: 0.4995045\n",
      "[2, 27201] loss: 0.5061086\n",
      "[2, 27521] loss: 0.5244870\n",
      "[2, 27841] loss: 0.4934166\n",
      "[2, 28161] loss: 0.5095280\n",
      "[2, 28481] loss: 0.4744594\n",
      "[2, 28801] loss: 0.5252087\n",
      "[2, 29121] loss: 0.5543906\n",
      "[2, 29441] loss: 0.5187867\n",
      "[2, 29761] loss: 0.5619135\n",
      "[2, 30081] loss: 0.5178428\n",
      "[2, 30401] loss: 0.5916917\n",
      "[2, 30721] loss: 0.4986352\n",
      "[2, 31041] loss: 0.6392975\n",
      "[2, 31361] loss: 0.6961053\n",
      "[2, 31681] loss: 0.6462455\n",
      "[2, 32001] loss: 0.6872158\n",
      "[2, 32321] loss: 0.6745972\n",
      "[2, 32641] loss: 0.6779188\n",
      "[2, 32961] loss: 0.6508475\n",
      "[2, 33281] loss: 0.6872037\n",
      "[2, 33601] loss: 0.6785725\n",
      "[2, 33921] loss: 0.6683825\n",
      "[2, 34241] loss: 0.6808823\n",
      "[2, 34561] loss: 0.6792924\n",
      "[2, 34881] loss: 0.6629684\n",
      "[2, 35201] loss: 0.6796445\n",
      "[2, 35521] loss: 0.6758254\n",
      "[2, 35841] loss: 0.6890218\n",
      "[2, 36161] loss: 0.6736248\n",
      "[2, 36481] loss: 0.6729127\n",
      "[2, 36801] loss: 0.6732609\n",
      "[2, 37121] loss: 0.6550880\n",
      "[2, 37441] loss: 0.6880996\n",
      "[2, 37761] loss: 0.6676401\n",
      "[2, 38081] loss: 0.6777765\n",
      "[2, 38401] loss: 0.6845331\n",
      "[2, 38721] loss: 0.6680823\n",
      "[2, 39041] loss: 0.6806097\n",
      "[2, 39361] loss: 0.7054801\n",
      "[2, 39681] loss: 0.7018335\n",
      "[2, 40001] loss: 0.6947574\n",
      "[2, 40321] loss: 0.6873476\n",
      "[2, 40641] loss: 0.6834419\n",
      "[2, 40961] loss: 0.6883599\n",
      "[2, 41281] loss: 0.6833551\n",
      "[2, 41601] loss: 0.6959362\n",
      "[2, 41921] loss: 0.6855765\n",
      "[2, 42241] loss: 0.6763103\n",
      "[2, 42561] loss: 0.6854546\n",
      "[2, 42881] loss: 0.6804516\n",
      "[2, 43201] loss: 0.6954345\n",
      "[2, 43521] loss: 0.7024839\n",
      "[2, 43841] loss: 0.6894834\n",
      "[2, 44161] loss: 0.6864846\n",
      "[2, 44481] loss: 0.6838187\n",
      "[2, 44801] loss: 0.6768207\n",
      "[2, 45121] loss: 0.6921691\n",
      "[2, 45441] loss: 0.6964513\n",
      "[2, 45761] loss: 0.6898811\n",
      "[2, 46081] loss: 0.6917134\n",
      "[2, 46401] loss: 0.6909612\n",
      "[2, 46721] loss: 0.6918728\n",
      "[3,   641] loss: 1.3483040\n",
      "[3,   961] loss: 0.6002901\n",
      "[3,  1281] loss: 0.6094232\n",
      "[3,  1601] loss: 0.6389289\n",
      "[3,  1921] loss: 0.6002867\n",
      "[3,  2241] loss: 0.6508288\n",
      "[3,  2561] loss: 0.6192804\n",
      "[3,  2881] loss: 0.6340679\n",
      "[3,  3201] loss: 0.6397952\n",
      "[3,  3521] loss: 0.6297568\n",
      "[3,  3841] loss: 0.6285440\n",
      "[3,  4161] loss: 0.6000766\n",
      "[3,  4481] loss: 0.6355622\n",
      "[3,  4801] loss: 0.6362984\n",
      "[3,  5121] loss: 0.5928032\n",
      "[3,  5441] loss: 0.6595592\n",
      "[3,  5761] loss: 0.6246734\n",
      "[3,  6081] loss: 0.6124571\n",
      "[3,  6401] loss: 0.6546603\n",
      "[3,  6721] loss: 0.6308659\n",
      "[3,  7041] loss: 0.6164316\n",
      "[3,  7361] loss: 0.6080237\n",
      "[3,  7681] loss: 0.6382043\n",
      "[3,  8001] loss: 0.6477042\n",
      "[3,  8321] loss: 0.6103059\n",
      "[3,  8641] loss: 0.6321119\n",
      "[3,  8961] loss: 0.6114756\n",
      "[3,  9281] loss: 0.6061729\n",
      "[3,  9601] loss: 0.6179812\n",
      "[3,  9921] loss: 0.5991595\n",
      "[3, 10241] loss: 0.6285466\n",
      "[3, 10561] loss: 0.6106910\n",
      "[3, 10881] loss: 0.5939523\n",
      "[3, 11201] loss: 0.6071278\n",
      "[3, 11521] loss: 0.6145287\n",
      "[3, 11841] loss: 0.6219308\n",
      "[3, 12161] loss: 0.6130916\n",
      "[3, 12481] loss: 0.6472552\n",
      "[3, 12801] loss: 0.6374327\n",
      "[3, 13121] loss: 0.6262153\n",
      "[3, 13441] loss: 0.6056362\n",
      "[3, 13761] loss: 0.6430243\n",
      "[3, 14081] loss: 0.6343628\n",
      "[3, 14401] loss: 0.6002142\n",
      "[3, 14721] loss: 0.6252673\n",
      "[3, 15041] loss: 0.6674499\n",
      "[3, 15361] loss: 0.6299999\n",
      "[3, 15681] loss: 0.6349052\n",
      "[3, 16001] loss: 0.6406047\n",
      "[3, 16321] loss: 0.6461551\n",
      "[3, 16641] loss: 0.6307073\n",
      "[3, 16961] loss: 0.6134519\n",
      "[3, 17281] loss: 0.6103401\n",
      "[3, 17601] loss: 0.6401454\n",
      "[3, 17921] loss: 0.6077572\n",
      "[3, 18241] loss: 0.6067494\n",
      "[3, 18561] loss: 0.6559783\n",
      "[3, 18881] loss: 0.6188520\n",
      "[3, 19201] loss: 0.6270408\n",
      "[3, 19521] loss: 0.5545685\n",
      "[3, 19841] loss: 0.6248843\n",
      "[3, 20161] loss: 0.6223548\n",
      "[3, 20481] loss: 0.6261687\n",
      "[3, 20801] loss: 0.6996202\n",
      "[3, 21121] loss: 0.6702731\n",
      "[3, 21441] loss: 0.6620158\n",
      "[3, 21761] loss: 0.6838228\n",
      "[3, 22081] loss: 0.6687303\n",
      "[3, 22401] loss: 0.6455829\n",
      "[3, 22721] loss: 0.6608343\n",
      "[3, 23041] loss: 0.6748785\n",
      "[3, 23361] loss: 0.6517972\n",
      "[3, 23681] loss: 0.6576520\n",
      "[3, 24001] loss: 0.6620097\n",
      "[3, 24321] loss: 0.6368667\n",
      "[3, 24641] loss: 0.6839792\n",
      "[3, 24961] loss: 0.6431679\n",
      "[3, 25281] loss: 0.6636472\n",
      "[3, 25601] loss: 0.6746035\n",
      "[3, 25921] loss: 0.6514955\n",
      "[3, 26241] loss: 0.6415816\n",
      "[3, 26561] loss: 0.6086157\n",
      "[3, 26881] loss: 0.4920395\n",
      "[3, 27201] loss: 0.5018195\n",
      "[3, 27521] loss: 0.5212172\n",
      "[3, 27841] loss: 0.4901124\n",
      "[3, 28161] loss: 0.5078674\n",
      "[3, 28481] loss: 0.4726908\n",
      "[3, 28801] loss: 0.5239743\n",
      "[3, 29121] loss: 0.5550430\n",
      "[3, 29441] loss: 0.5186757\n",
      "[3, 29761] loss: 0.5622400\n",
      "[3, 30081] loss: 0.5164632\n",
      "[3, 30401] loss: 0.5914120\n",
      "[3, 30721] loss: 0.4973564\n",
      "[3, 31041] loss: 0.6349582\n",
      "[3, 31361] loss: 0.6898496\n",
      "[3, 31681] loss: 0.6452727\n",
      "[3, 32001] loss: 0.6836972\n",
      "[3, 32321] loss: 0.6731205\n",
      "[3, 32641] loss: 0.6776832\n",
      "[3, 32961] loss: 0.6505418\n",
      "[3, 33281] loss: 0.6866290\n",
      "[3, 33601] loss: 0.6784750\n",
      "[3, 33921] loss: 0.6688591\n",
      "[3, 34241] loss: 0.6808412\n",
      "[3, 34561] loss: 0.6785686\n",
      "[3, 34881] loss: 0.6618778\n",
      "[3, 35201] loss: 0.6793201\n",
      "[3, 35521] loss: 0.6751873\n",
      "[3, 35841] loss: 0.6892163\n",
      "[3, 36161] loss: 0.6735969\n",
      "[3, 36481] loss: 0.6733868\n",
      "[3, 36801] loss: 0.6738007\n",
      "[3, 37121] loss: 0.6548725\n",
      "[3, 37441] loss: 0.6881486\n",
      "[3, 37761] loss: 0.6679386\n",
      "[3, 38081] loss: 0.6773101\n",
      "[3, 38401] loss: 0.6853114\n",
      "[3, 38721] loss: 0.6678747\n",
      "[3, 39041] loss: 0.6808038\n",
      "[3, 39361] loss: 0.7045813\n",
      "[3, 39681] loss: 0.7011083\n",
      "[3, 40001] loss: 0.6942056\n",
      "[3, 40321] loss: 0.6873380\n",
      "[3, 40641] loss: 0.6838701\n",
      "[3, 40961] loss: 0.6879562\n",
      "[3, 41281] loss: 0.6826677\n",
      "[3, 41601] loss: 0.6954039\n",
      "[3, 41921] loss: 0.6860385\n",
      "[3, 42241] loss: 0.6764678\n",
      "[3, 42561] loss: 0.6852283\n",
      "[3, 42881] loss: 0.6801475\n",
      "[3, 43201] loss: 0.6950832\n",
      "[3, 43521] loss: 0.7019038\n",
      "[3, 43841] loss: 0.6896164\n",
      "[3, 44161] loss: 0.6859926\n",
      "[3, 44481] loss: 0.6836002\n",
      "[3, 44801] loss: 0.6774087\n",
      "[3, 45121] loss: 0.6928688\n",
      "[3, 45441] loss: 0.6962091\n",
      "[3, 45761] loss: 0.6895604\n",
      "[3, 46081] loss: 0.6911437\n",
      "[3, 46401] loss: 0.6901748\n",
      "[3, 46721] loss: 0.6919881\n"
     ]
    }
   ],
   "source": [
    "# Test using one condition \n",
    "\n",
    "feature_combination = [\"socio-demographics\", \"health\", \"social\", \"neighbourhood\"]\n",
    "CSV_raw = \"../ML_social.csv\"\n",
    "    \n",
    "loss_log = exp_one_target_one_arch(use_feature_categories=feature_combination, \n",
    "                        csv_path=CSV_raw,\n",
    "                        target_index=0, minimal_length=4, \n",
    "                        arch=\"RNN\", learning_rate=0.001, momentum=0.9,\n",
    "                        batch_size=32, print_batch_size=32*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/forrest/repos/AgeML/RNN/fixed_length.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.12.180/home/forrest/repos/AgeML/RNN/fixed_length.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m CSV_raw \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m../ML_social.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.12.180/home/forrest/repos/AgeML/RNN/fixed_length.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfor\u001b[39;00m use_feature_categories \u001b[39min\u001b[39;00m feature_combinations:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.12.180/home/forrest/repos/AgeML/RNN/fixed_length.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining for \u001b[39m\u001b[39m{\u001b[39;00muse_feature_categories\u001b[39m}\u001b[39;00m\u001b[39m on target \u001b[39m\u001b[39m{\u001b[39;00mtarget_index\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.12.180/home/forrest/repos/AgeML/RNN/fixed_length.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m         \u001b[39m# loss_log = exp_one_target_one_arch(use_feature_categories=feature_combination, \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.12.180/home/forrest/repos/AgeML/RNN/fixed_length.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m         \u001b[39m#                 csv_path=CSV_raw,\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.12.180/home/forrest/repos/AgeML/RNN/fixed_length.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m         \u001b[39m#                 target_index=0, minimal_length=4, \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.12.180/home/forrest/repos/AgeML/RNN/fixed_length.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m         \u001b[39m#                 arch=\"RNN\", learning_rate=0.001, momentum=0.9,\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.12.180/home/forrest/repos/AgeML/RNN/fixed_length.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m         \u001b[39m#                 batch_size=32, print_batch_size=32*10)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.12.180/home/forrest/repos/AgeML/RNN/fixed_length.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m         loss_log \u001b[39m=\u001b[39m exp_one_target_one_arch(use_feature_categories\u001b[39m=\u001b[39muse_feature_categories, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.12.180/home/forrest/repos/AgeML/RNN/fixed_length.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m                         csv_path\u001b[39m=\u001b[39mCSV_raw,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.12.180/home/forrest/repos/AgeML/RNN/fixed_length.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m                         target_index\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, minimal_length\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.12.180/home/forrest/repos/AgeML/RNN/fixed_length.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m                         arch\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRNN\u001b[39m\u001b[39m\"\u001b[39m, learning_rate\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m, momentum\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.12.180/home/forrest/repos/AgeML/RNN/fixed_length.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m                         batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, print_batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m\u001b[39m*\u001b[39m\u001b[39m300\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'target_index' is not defined"
     ]
    }
   ],
   "source": [
    "feature_combinations = [\n",
    "    [\"socio-demographics\"],\n",
    "    [\"health\"],\n",
    "    [\"social\"],\n",
    "    [\"neighbourhood\"],\n",
    "    # [\"socio-demographics\", \"health\", \"social\", \"neighbourhood\"]\n",
    "    [\"health\", \"social\"],\n",
    "    [\"social\", \"neighbourhood\"],\n",
    "    [\"socio-demographics\", \"health\"]\n",
    "]\n",
    "\n",
    "CSV_raw = \"../ML_social.csv\"\n",
    "for use_feature_categories in feature_combinations:\n",
    "    for target_index in range(4):\n",
    "        print(f\"Training for {use_feature_categories} on target {target_index}\")\n",
    "\n",
    "        # loss_log = exp_one_target_one_arch(use_feature_categories=feature_combination, \n",
    "        #                 csv_path=CSV_raw,\n",
    "        #                 target_index=0, minimal_length=4, \n",
    "        #                 arch=\"RNN\", learning_rate=0.001, momentum=0.9,\n",
    "        #                 batch_size=32, print_batch_size=32*10)\n",
    "\n",
    "        loss_log = exp_one_target_one_arch(use_feature_categories=use_feature_categories, \n",
    "                        csv_path=CSV_raw,\n",
    "                        target_index=0, minimal_length=4, \n",
    "                        arch=\"RNN\", learning_rate=0.01, momentum=0.9,\n",
    "                        batch_size=32, print_batch_size=32*300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
